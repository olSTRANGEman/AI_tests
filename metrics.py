import subprocess
import json
import re
import os
import tempfile
from collections import defaultdict
from typing import List, Set, Tuple, Dict
import ast
import requests
import yaml
import html
from io import StringIO
from radon.visitors import ComplexityVisitor
from pylint.lint import Run
from pylint.reporters.text import TextReporter

# pip install radon pylint requests pyyaml pytest-json-report

# –§–∞–π–ª —Å —Ç–µ—Å—Ç–∞–º–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ .py)
TEST_FILEs = ["sub_test/subtest.py"]

# –ë–∞–∑–æ–≤—ã–π URL –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤
BASE_URL = "https://petstore.swagger.io/v2"

# –ü—É—Ç–∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏
ENDPOINTS = [
    "/swagger.json",
    "/openapi.json",
    "/swagger.yaml",
    "/openapi.yaml",
    "/v2/swagger.json",
]

ACCEPTABLE_CODES = [400, 401, 403, 404, 405, 409, 415, 422]

def run_pytest_json(test_name: str = None, report_file: str = "report.json") -> dict:
    cmd = [
        "pytest",
        TEST_FILE if not test_name else f"{TEST_FILE}::{test_name}",
        "--json-report",
        f"--json-report-file={report_file}",
        "--maxfail=0"
    ]
    proc = subprocess.run(cmd, capture_output=True, text=True)
    if proc.returncode not in (0, 1):
        print(f"‚ùå pytest –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –∫–æ–¥–æ–º {proc.returncode}")
        print(proc.stderr)
        return {}
    try:
        with open(report_file, encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω {report_file}")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ –ø–ª–∞–≥–∏–Ω pytest-json-report: pip install pytest-json-report")
        return {}


def extract_longrepr_text(lr) -> str:
    if isinstance(lr, str):
        return html.unescape(lr)
    elif isinstance(lr, dict):
        return html.unescape(lr.get("message", "") or lr.get("repr", "") or str(lr))
    elif isinstance(lr, list):
        return "\n".join(str(item) for item in lr if isinstance(item, str))
    else:
        return str(lr)
    

def get_pass_fail_rate_firs(report: dict) -> Tuple[int, int]:
    """
    –í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–µ—Å—Ç–∞–º:
     - total, corrected_cnt/%, suspicious_cnt/% 
     - Adjusted Pass Rate
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (passed, failed).
    """
    tests = report.get("tests", [])
    total = len(tests)

    adjusted = []
    suspicious = []

    for t in tests:
        if t.get("outcome") != "passed":
            text = extract_longrepr_text(t["call"].get("longrepr", ""))
            nid = t["nodeid"]

            # 1) assert 200 == CODE
            if any(re.search(rf"assert\s+200\s+==\s+{c}", text) for c in ACCEPTABLE_CODES):
                adjusted.append(nid)
                continue

            # 2) assert 200 in [C1, C2, ...]
            m = re.search(r"assert\s+200\s+in\s+[\[\(]([^\]\)]+)[\]\)]", text)
            if m:
                try:
                    codes = [int(c.strip()) for c in m.group(1).split(",")]
                    if any(c in ACCEPTABLE_CODES for c in codes):
                        adjusted.append(nid)
                        continue
                except ValueError:
                    pass

            # 3) assert 4xx == 200
            if re.search(r"assert\s+4\d\d\s+==\s+200", text):
                suspicious.append(nid)

    corrected_cnt   = len(adjusted)
    suspicious_cnt  = len(suspicious)
    passed          = sum(1 for t in tests if t.get("outcome") == "passed" or t["nodeid"] in adjusted)
    failed          = total - passed
    total_nonzero   = total or 1

    print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã get_pass_fail_rate:")
    print(f"üéØ Pass Rate: {passed}/{total} ({passed/total_nonzero:.1%})")

    return passed, failed

def get_pass_fail_rate_sec(report: dict) -> Tuple[int, int]:
    """
    –í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–µ—Å—Ç–∞–º:
     - total, corrected_cnt/%, suspicious_cnt/% 
     - Adjusted Pass Rate
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (passed, failed).
    """
    tests = report.get("tests", [])
    total = len(tests)

    adjusted = []
    suspicious = []

    for t in tests:
        if t.get("outcome") != "passed":
            text = extract_longrepr_text(t["call"].get("longrepr", ""))
            nid = t["nodeid"]

            # 1) assert 200 == CODE
            if any(re.search(rf"assert\s+200\s+==\s+{c}", text) for c in ACCEPTABLE_CODES):
                adjusted.append(nid)
                continue

            # 2) assert 200 in [C1, C2, ...]
            m = re.search(r"assert\s+200\s+in\s+[\[\(]([^\]\)]+)[\]\)]", text)
            if m:
                try:
                    codes = [int(c.strip()) for c in m.group(1).split(",")]
                    if any(c in ACCEPTABLE_CODES for c in codes):
                        adjusted.append(nid)
                        continue
                except ValueError:
                    pass

            # 3) assert 4xx == 200
            if re.search(r"assert\s+4\d\d\s+==\s+200", text):
                suspicious.append(nid)

    corrected_cnt   = len(adjusted)
    suspicious_cnt  = len(suspicious)
    passed          = sum(1 for t in tests if t.get("outcome") == "passed" or t["nodeid"] in adjusted)
    failed          = total - passed
    total_nonzero   = total or 1

    print(f"‚úîÔ∏è –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–æ (200==4xx / in [...]): {corrected_cnt} ({corrected_cnt/total_nonzero*100:.1f}%)")
    print(f"üü° –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö (4xx==200): {suspicious_cnt} ({suspicious_cnt/total_nonzero*100:.1f}%)")

    return passed, failed


def print_pass_fail_details(report: dict) -> None:
    """
    –ü–µ—á–∞—Ç–∞–µ—Ç —Å–ø–∏—Å–∫–∏:
     - —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã (assert 200==4xx –∏ in [...])
     - –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã (assert 4xx(500)==200)
    """
    tests = report.get("tests", [])
    adjusted = []
    suspicious = []

    for t in tests:
        if t.get("outcome") != "passed":
            text = extract_longrepr_text(t["call"].get("longrepr", ""))
            nid = t["nodeid"]

            if any(re.search(rf"assert\s+200\s+==\s+{c}", text) for c in ACCEPTABLE_CODES):
                adjusted.append(nid)
                continue
            m = re.search(r"assert\s+200\s+in\s+[\[\(]([^\]\)]+)[\]\)]", text)
            if m:
                try:
                    codes = [int(c.strip()) for c in m.group(1).split(",")]
                    if any(c in ACCEPTABLE_CODES for c in codes):
                        adjusted.append(nid)
                        continue
                except ValueError:
                    pass
            if re.search(r"assert\s+4\d\d\s+==\s+200", text):
                suspicious.append(nid)
            if re.search(r"assert\s+5\d\d\s+==\s+200", text):
                suspicious.append(nid)

    if adjusted:
        print("\n‚úîÔ∏è –°–ø–∏—Å–æ–∫ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤:")
        for nid in adjusted:
            print(f"   - {nid}")
    if suspicious:
        print("\nüü° –°–ø–∏—Å–æ–∫ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤:")
        for nid in suspicious:
            print(f"   - {nid}")







def extract_used_endpoints(test_file: str) -> Set[Tuple[str, str]]:
    text = open(test_file, encoding='utf-8').read()

    # 1) –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö URL
    url_var_pattern = re.compile(
        r'(\w+)\s*=\s*'
        r'(?:f?["\']\{?BASE_URL\}?/([^"\'\?]+)(?:\?[^"\']*)?["\']'
        r'|BASE_URL\s*\+\s*["\']([^"\\?]+)["\'])',
        re.IGNORECASE
    )
    url_vars: Dict[str, str] = {}
    for m in url_var_pattern.finditer(text):
        var = m.group(1)
        raw = m.group(2) or m.group(3)
        url_vars[var] = raw.strip('/')

    # 2) –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä—è–º—ã—Ö –≤—ã–∑–æ–≤–æ–≤
    direct_patterns = [
        re.compile(
            r'requests\.(get|post|put|delete)\s*\(\s*f?["\']'
            r'(?:\{?BASE_URL\}?/)?([^"\'\?]+?)(?:\?[^"\']*)?["\']\s*(?:,.*)?\)',
            re.IGNORECASE
        ),
        re.compile(
            r'send_request\(\s*["\'](GET|POST|PUT|DELETE)["\']\s*,\s*f?["\']'
            r'(?:\{?BASE_URL\}?/)?([^"\'\?]+)(?:\?[^"\']*)?["\']',
            re.IGNORECASE
        ),
        re.compile(
            r'send_request\(\s*["\'](GET|POST|PUT|DELETE)["\']\s*,\s*'
            r'BASE_URL\s*\+\s*["\']([^"\\?]+)["\']',
            re.IGNORECASE
        ),
    ]
    var_call_pattern = re.compile(
        r'(?:requests\.(get|post|put|delete)|send_request)\s*\(\s*'
        r'(?:["\'](GET|POST|PUT|DELETE)["\']\s*,\s*)?'
        r'(\w+)',
        re.IGNORECASE
    )

    fixed_segments = {
        'pet', 'store', 'order', 'user',
        'findByStatus', 'findByTags',
        'inventory', 'login', 'logout'
    }
    endpoints: Set[Tuple[str, str]] = set()

    # 3) –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä—è–º—ã—Ö –≤—ã–∑–æ–≤–æ–≤
    for pat in direct_patterns:
        for method, raw in pat.findall(text):
            m = method.upper()
            core = raw.strip('/')
            segs = [seg if seg in fixed_segments else '{param}' for seg in core.split('/')]
            endpoints.add((m, '/' + '/'.join(segs)))

    # 4) –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–∑–æ–≤–æ–≤ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    for m1, m2, var in var_call_pattern.findall(text):
        method = (m1 or m2).upper()
        if var in url_vars:
            core = url_vars[var]
            segs = [seg if seg in fixed_segments else '{param}' for seg in core.split('/')]
            endpoints.add((method, '/' + '/'.join(segs)))

    return endpoints


def parse_openapi(spec_data: bytes | str) -> dict:
    if isinstance(spec_data, bytes):
        spec_data = spec_data.decode('utf-8')
    try:
        if spec_data.lstrip().startswith('{'):
            return json.loads(spec_data)
        return yaml.safe_load(spec_data)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
        return {}


def get_pass_fail_rate(report: dict) -> Tuple[int, int]:
    tests = report.get("tests", [])
    total = len(tests)

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —Å–±–æ—Ä–∞
    adjusted_4xx_from_200 = []       # assert 200 == 4xx –∏–ª–∏ in [...]
    suspicious_4xx_eq_200 = []       # assert 4xx == 200
    all_adjusted = []

    for t in tests:
        if t.get("outcome") != "passed":
            call_data = t.get("call", {})
            raw_longrepr = call_data.get("longrepr", "")
            text = extract_longrepr_text(raw_longrepr)

            matched = False
            nodeid = t["nodeid"]

            # –¢–∏–ø 1: assert 200 == 4xx
            for code in ACCEPTABLE_CODES:
                if re.search(rf"assert\s+200\s+==\s+{code}", text):
                    adjusted_4xx_from_200.append(nodeid)
                    matched = True
                    break

            # –¢–∏–ø 2: assert 200 in [400, 404] –∏–ª–∏ (400, 415)
            if not matched:
                match = re.search(r"assert\s+200\s+in\s+[\[\(]([^\]\)]+)[\]\)]", text)
                if match:
                    codes_str = match.group(1)
                    try:
                        codes = [int(c.strip()) for c in codes_str.split(",")]
                        if any(code in ACCEPTABLE_CODES for code in codes):
                            adjusted_4xx_from_200.append(nodeid)
                            matched = True
                    except ValueError:
                        pass

            # –¢–∏–ø 3: assert 4xx == 200 (–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π)
            if not matched:
                match = re.search(r"assert\s+(4\d\d)\s+==\s+200", text)
                if match:
                    suspicious_4xx_eq_200.append(nodeid)

            # –í—Å–µ –∑–∞—á—Ç—ë–Ω–Ω—ã–µ –∫–∞–∫ –ø—Ä–æ–π–¥–µ–Ω–Ω—ã–µ
            if matched:
                all_adjusted.append(nodeid)

    # –ü–æ–¥—Å—á—ë—Ç
    passed = sum(1 for t in tests if t.get("outcome") == "passed" or t["nodeid"] in all_adjusted)
    failed = total - passed

    # –û—Ç—á—ë—Ç
    print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
    print(f"‚úÖ –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total}")
    print(f"‚úîÔ∏è –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ (assert 200 == 4xx / in [...]): {len(adjusted_4xx_from_200)} "
          f"({len(adjusted_4xx_from_200) / total * 100:.1f}%)")
    print(f"üü° –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ (assert 4xx == 200): {len(suspicious_4xx_eq_200)} "
          f"({len(suspicious_4xx_eq_200) / total * 100:.1f}%)")
    print(f"‚úÖ Adjusted Pass Rate: {passed}/{total} ({passed / total:.1%})")

    return passed, failed



def measure_api_coverage(
    used_status_codes: Dict[Tuple[str, str], Set[str]],
    openapi_spec: dict
) -> None:
    """
    –°—á–∏—Ç–∞–µ—Ç, —Å–∫–æ–ª—å–∫–æ –∏–∑ –≤—Å–µ—Ö –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö –≤ spec endpoints
    –±—ã–ª–∏ ¬´—á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–∫—Ä—ã—Ç—ã¬ª ‚Äî
      1) –ª–∏–±–æ –ø–æ–ª—É—á–∏–ª–∏ —Ö–æ—Ç—å –æ–¥–∏–Ω —Å—Ç–∞—Ç—É—Å-–∫–æ–¥ (–∫–ª—é—á–∏ used_status_codes –ø–µ—Ä–µ—Å–µ–∫–∞—é—Ç—Å—è —Å Specification_codes),
      2) –ª–∏–±–æ —ç—Ç–æ ¬´–ø—É—Å—Ç—ã–µ¬ª endpoints (–≤ spec –Ω–µ—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö response-–∫–æ–¥–æ–≤, —Ç–æ–ª—å–∫–æ default).

    –í—ã–≤–æ–¥–∏—Ç:
      üß™ API partly Endpoint Coverage: X/Y (Z%)
    """
    # 1) –°–æ–±–∏—Ä–∞–µ–º –∏–∑ spec –≤—Å–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –∏ –∏—Ö —Ä–µ–∞–ª—å–Ω—ã–µ –∫–æ–¥—ã (–±–µ–∑ 'default'):
    Specification_codes: Dict[Tuple[str, str], Set[str]] = {}
    for raw_path, methods in openapi_spec.get('paths', {}).items():
        norm = re.sub(r'\{\w+\}', '{param}', raw_path)
        for method, op in methods.items():
            m = method.upper()
            if m not in {'GET', 'POST', 'PUT', 'DELETE', 'PATCH'}:
                continue
            codes = {
                code for code in op.get('responses', {})
                if code.lower() != 'default'
            }
            Specification_codes[(m, norm)] = codes

    total = len(Specification_codes)

    # 2a) –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º: –µ—Å—Ç—å –∏ –≤ used_status_codes, –∏ –≤ Specification_codes
    used_eps = {ep for ep in used_status_codes if ep in Specification_codes and used_status_codes[ep]}

    # 2b) ¬´–ü—É—Å—Ç—ã–µ¬ª endpoints (—Ç–æ–ª—å–∫–æ default)
    empty_eps = {ep for ep, codes in Specification_codes.items() if not codes}

    # 3) ¬´–ß–∞—Å—Ç–∏—á–Ω–æ –ø–æ–∫—Ä—ã—Ç—ã¬ª = union —ç—Ç–∏—Ö –¥–≤—É—Ö –º–Ω–æ–∂–µ—Å—Ç–≤
    covered_eps = used_eps | empty_eps
    covered = len(covered_eps)

    # 4) –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç
    pct = (covered / total * 100) if total else 0.0

    print(f"\nüß™ API partly Endpoint Coverage: {covered}/{total} ({pct:.1f}%)")




def extract_used_status_codes(test_file: str) -> Dict[tuple[str, str], Set[str]]:
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
    with open(test_file, encoding='utf-8') as f:
        lines = f.read().splitlines()

    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—ã–∑–æ–≤–æ–≤ requests
    call_re = re.compile(
        r'requests\.(get|post|put|delete)\s*\(\s*f?["\']\{BASE_URL\}/([^"\'\?]+?)(?:\?[^"\']*)?["\']',
        re.IGNORECASE
    )
    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å-–∫–æ–¥–æ–≤
    assert_re = re.compile(r'assert\s+(?P<var>\w+)\.status_code\s*==\s*(?P<code>\d{3})')

    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    used_status_codes: Dict[tuple[str, str], Set[str]] = {}

    # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –ø—É—Ç–∏ (–Ω–µ –∑–∞–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ {param})
    fixed_segments = {'pet', 'store', 'order', 'user', 'findByStatus', 'uploadImage', 'inventory', 'login', 'logout', 'createWithArray', 'createWithList'}

    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º —Ñ–∞–π–ª–∞
    for idx, line in enumerate(lines):
        # –ò—â–µ–º –≤—ã–∑–æ–≤ HTTP
        call_match = call_re.search(line)
        if call_match:
            method = call_match.group(1).upper()  # –ù–∞–ø—Ä–∏–º–µ—Ä, "POST"
            raw_path = call_match.group(2)        # –ù–∞–ø—Ä–∏–º–µ—Ä, "pet" –∏–ª–∏ "pet/9999"

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å
            segs = raw_path.split('/')
            norm_segs = [seg if seg in fixed_segments else '{param}' for seg in segs]
            norm_path = '/' + '/'.join(norm_segs)  # –ù–∞–ø—Ä–∏–º–µ—Ä, "/pet" –∏–ª–∏ "/pet/{param}"

            # –ò—â–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –≤ —Å–ª–µ–¥—É—é—â–µ–π –∏–ª–∏ —á–µ—Ä–µ–∑ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫–µ
            for offset in (1, 2):
                if idx + offset < len(lines):
                    assert_match = assert_re.search(lines[idx + offset])
                    if assert_match:
                        code = assert_match.group('code')  # –ù–∞–ø—Ä–∏–º–µ—Ä, "200"
                        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª–æ–≤–∞—Ä—å
                        key = (method, norm_path)
                        used_status_codes.setdefault(key, set()).add(code)
                        break  # –ù–∞—à–ª–∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ, –¥–∞–ª—å—à–µ –Ω–µ –∏—â–µ–º

    return used_status_codes



def extract_openapi_responses(spec: dict) -> Dict[Tuple[str, str], Set[str]]:
    responses = {}
    for path, methods in spec.get('paths', {}).items():
        norm_path = re.sub(r'\{\w+\}', '{param}', path)
        for method, op in methods.items():
            method = method.upper()
            if method not in {'GET', 'POST', 'PUT', 'DELETE', 'PATCH'}:
                continue
            codes = {
                code for code in op.get('responses', {}).keys()
                if code.isdigit()
            }
            responses[(method, norm_path)] = codes
    return responses


def count_tests(test_file: str) -> int:
    """
    –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤ –≤ —Ñ–∞–π–ª–µ (—Ñ—É–Ω–∫—Ü–∏–π, –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å 'def test_').

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        test_file: –ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—É.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤.
    """
    try:
        with open(test_file, encoding='utf-8') as f:
            return sum(1 for line in f if line.strip().startswith('def test_'))
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ —Ç–µ—Å—Ç–æ–≤: {e}")
        return 1  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 1, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
    
def analyze_unSpecification_status_codes(
    test_file: str,
    openapi_spec: dict
) -> List[Tuple[str, str, str, str]]:
    """
    –ü–µ—á–∞—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–≤–æ–¥–Ω—É—é –º–µ—Ç—Ä–∏–∫—É:
      ‚ùó –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–µ–∑–∞—è–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—É—Å-–∫–æ–¥–æ–≤: X / total_tests (Y%)
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ unSpecification_cases –¥–ª—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏.
    """
    Specification = extract_openapi_responses(openapi_spec)
    used     = extract_used_status_codes(test_file)
    total    = count_tests(test_file)

    unSpecification = []
    for (m, path), codes in used.items():
        exp_set = Specification.get((m, path), set())
        for code in codes:
            if code not in exp_set:
                unSpecification.append((m, path, code, ",".join(sorted(exp_set))))

    cnt = len(unSpecification)
    pct = cnt/ (total or 1) * 100
    print(f"‚ùó –ù–µ–∑–∞—è–≤–ª–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—É—Å-–∫–æ–¥—ã: {cnt} / {total} ({pct:.1f}%)\n")

    return unSpecification



def analyze_unSpecification_status_det(
    test_file: str,
    openapi_spec: dict
) -> None:
    """
    –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è analyze_unSpecification_status_metrics:
      - —Å–ø–∏—Å–æ–∫ –Ω–µ–∑–∞—è–≤–ª–µ–Ω–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
    """
    # –ø—Ä—è–º–æ –¥—É–±–ª–∏—Ä—É–µ–º –ª–æ–≥–∏–∫—É, –±–µ–∑ –≤—ã–∑–æ–≤–∞ metrics-—Ñ—É–Ω–∫—Ü–∏–∏
    Specification = extract_openapi_responses(openapi_spec)
    used     = extract_used_status_codes(test_file)

    unSpecification = []
    for (method, path), codes in used.items():
        exp_set = Specification.get((method, path), set())
        for code in codes:
            if code not in exp_set:
                unSpecification.append((method, path, code, ",".join(sorted(exp_set))))

    if unSpecification:
        print("\n‚ùó –°–ø–∏—Å–æ–∫ –Ω–µ–∑–∞—è–≤–ª–µ–Ω–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤:")
        for m, path, actual, exp in unSpecification:
            print(f"   - {m} {path}: –ø–æ–ª—É—á–∏–ª–∏ {actual}, –æ–∂–∏–¥–∞–ª–∏ [{exp or '-'}]")



def show_status_code_coverage_sec(
    used_status_codes: Dict[Tuple[str, str], Set[str]],
    openapi_spec: dict
) -> None:
    """
    –°—Ä–µ–¥–Ω–µ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º –≥—Ä–∞—Ñ.
    """
    import re

    # –°–æ–±–∏—Ä–∞–µ–º –æ–∂–∏–¥–∞–µ–º—ã–µ –∫–æ–¥—ã –ø–æ (METHOD, PATH)
    Specification_codes: Dict[Tuple[str, str], List[str]] = {}
    for raw_path, methods in openapi_spec.get('paths', {}).items():
        norm = re.sub(r'\{\w+\}', '{param}', raw_path)
        for http_method, op_obj in methods.items():
            m = http_method.upper()
            if m not in {'GET', 'POST', 'PUT', 'DELETE', 'PATCH'}:
                continue
            codes = list(op_obj.get('responses', {}).keys())
            Specification_codes[(m, norm)] = sorted(codes)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç—Ä–æ–∫: —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ path
    rows = []
    for (method, path) in sorted(Specification_codes.keys(), key=lambda x: (x[1], x[0])):
        exp_codes = Specification_codes[(method, path)]
        recv = sorted(used_status_codes.get((method, path), []))
        declared = set(exp_codes)
        actual_declared = [code for code in recv if code in declared]
        missing = sorted(declared - set(actual_declared))

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è —Å –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º default
        if declared == {'default'}:
            coverage_pct = 100.0
            actual_declared = ['default']
            missing = []
        else:
            coverage_pct = (len(actual_declared) / len(declared)) * 100.0 if declared else 0.0

        rows.append({
            'method': method,
            'path': path,
            'Specification': ','.join(exp_codes) or '-',
            'Expected': ','.join(recv) or '-',
            'missing': ','.join(missing) or '-',
            'coverage_pct': coverage_pct,
        })

    # –°—Ä–µ–¥–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
    avg_pct = sum(r['coverage_pct'] for r in rows) / len(rows) if rows else 0.0
    print(f"\n–°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è –ø–æ endpoint: {avg_pct:.1f}%")

    # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è –ø–æ —Å–µ–∫—Ü–∏—è–º
    section_stats: Dict[str, List[float]] = {}
    for r in rows:
        section = r['path'].split('/')[1] if '/' in r['path'] else r['path']
        section_stats.setdefault(section, []).append(r['coverage_pct'])
    print("–°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º:")
    for sec, pct_list in sorted(section_stats.items()):
        sec_avg = sum(pct_list) / len(pct_list)
        print(f"  - {sec}: {sec_avg:.1f}%")



def show_status_code_coverage(
    used_status_codes: Dict[Tuple[str, str], Set[str]],
    openapi_spec: dict
) -> None:
    """
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞ –∏–∑ openapi_spec —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç
    –æ–±—ä—è–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–¥—ã –æ—Ç–≤–µ—Ç–æ–≤ (–≤–∫–ª—é—á–∞—è 'default') –∏ —Ç–µ, —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –ø–æ–ª—É—á–∏–ª–∏,
    –∏ –≤—ã–≤–æ–¥–∏—Ç —Ç–∞–±–ª–∏—Ü—É —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π.
    """
    import re

    # –°–æ–±–∏—Ä–∞–µ–º –æ–∂–∏–¥–∞–µ–º—ã–µ –∫–æ–¥—ã –ø–æ (METHOD, PATH)
    Specification_codes: Dict[Tuple[str, str], List[str]] = {}
    for raw_path, methods in openapi_spec.get('paths', {}).items():
        norm = re.sub(r'\{\w+\}', '{param}', raw_path)
        for http_method, op_obj in methods.items():
            m = http_method.upper()
            if m not in {'GET', 'POST', 'PUT', 'DELETE', 'PATCH'}:
                continue
            codes = list(op_obj.get('responses', {}).keys())
            Specification_codes[(m, norm)] = sorted(codes)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç—Ä–æ–∫: —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ path
    rows = []
    for (method, path) in sorted(Specification_codes.keys(), key=lambda x: (x[1], x[0])):
        exp_codes = Specification_codes[(method, path)]
        recv = sorted(used_status_codes.get((method, path), []))
        declared = set(exp_codes)
        actual_declared = [code for code in recv if code in declared]
        missing = sorted(declared - set(actual_declared))

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è —Å –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º default
        if declared == {'default'}:
            coverage_pct = 100.0
            actual_declared = ['default']
            missing = []
        else:
            coverage_pct = (len(actual_declared) / len(declared)) * 100.0 if declared else 0.0

        rows.append({
            'method': method,
            'path': path,
            'Specification': ','.join(exp_codes) or '-',
            'Expected': ','.join(recv) or '-',
            'missing': ','.join(missing) or '-',
            'coverage_pct': coverage_pct,
        })



    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã —Å—Ç–æ–ª–±—Ü–æ–≤
    headers = ['METHOD', 'PATH', 'Specification', 'Expected', 'Missing', 'CovPct']
    col_widths = {h: len(h) for h in headers}
    for r in rows:
        col_widths['METHOD'] = max(col_widths['METHOD'], len(r['method']))
        col_widths['PATH'] = max(col_widths['PATH'], len(r['path']))
        col_widths['Specification'] = max(col_widths['Specification'], len(r['Specification']))
        col_widths['Expected'] = max(col_widths['Expected'], len(r['Expected']))
        col_widths['Missing'] = max(col_widths['Missing'], len(r['missing']))
        formatted_pct = f"{r['coverage_pct']:.1f}%"
        col_widths['CovPct'] = max(col_widths['CovPct'], len(formatted_pct))

    # –ü–µ—á–∞—Ç–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    header_line = (
        f"{headers[0]:<{col_widths['METHOD']}} | "
        f"{headers[1]:<{col_widths['PATH']}} | "
        f"{headers[2]:<{col_widths['Specification']}} | "
        f"{headers[3]:<{col_widths['Expected']}} | "
        f"{headers[4]:<{col_widths['Missing']}} | "
        f"{headers[5]:>{col_widths['CovPct']}}"
    )
    sep = ' | '.join('-' * col_widths[h] for h in headers)
    print()
    print(header_line)
    print(sep)

    # –ü–µ—á–∞—Ç–∞–µ–º —Å—Ç—Ä–æ–∫–∏
    for r in rows:
        print(
            f"{r['method']:<{col_widths['METHOD']}} | "
            f"{r['path']:<{col_widths['PATH']}} | "
            f"{r['Specification']:<{col_widths['Specification']}} | "
            f"{r['Expected']:<{col_widths['Expected']}} | "
            f"{r['missing']:<{col_widths['Missing']}} | "
            f"{r['coverage_pct']:>{col_widths['CovPct']}.1f}%"
        )




def extract_test_names(test_file: str) -> List[str]:
    names: List[str] = []
    for line in open(test_file, encoding='utf-8'):
        m = re.match(r'\s*def\s+(test_\w+)', line)
        if m:
            names.append(m.group(1))
    return names



def measure_full_status_coverage(
    used_status_codes: Dict[Tuple[str, str], Set[str]],
    openapi_spec: dict
) -> None:
    """
    –°—á–∏—Ç–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç endpoint-–æ–≤, —É –∫–æ—Ç–æ—Ä—ã—Ö –ø–æ–ª—É—á–µ–Ω—ã –í–°–ï –æ–∂–∏–¥–∞–µ–º—ã–µ –∫–æ–¥—ã –æ—Ç–≤–µ—Ç–æ–≤,
    –ø—Ä–∏—á—ë–º:
      - –µ—Å–ª–∏ –≤ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ —É endpoint-–∞ –Ω–µ –±—ã–ª–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞ (—Ç–æ–ª—å–∫–æ default),
        –æ–Ω —Ç–æ–∂–µ —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–∫—Ä—ã—Ç—ã–º;
      - –∏–Ω–∞—á–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ Specification_codes ‚äÜ used_status_codes.
    
    –í—ã–≤–æ–¥–∏—Ç:
      API Endpoint Coverage: X/Y (Z%)
    """
    # 1) –°–æ–±–∏—Ä–∞–µ–º –∏–∑ spec –≤—Å–µ endpoint-—ã + —Ä–µ–∞–ª—å–Ω—ã–µ –∫–æ–¥—ã (–±–µ–∑ 'default')
    Specification: Dict[Tuple[str, str], Set[str]] = {}
    for raw_path, methods in openapi_spec.get('paths', {}).items():
        norm = re.sub(r'\{\w+\}', '{param}', raw_path)
        for method, op in methods.items():
            m = method.upper()
            if m not in {'GET','POST','PUT','DELETE','PATCH'}:
                continue
            codes = {
                code for code in op.get('responses', {})
                if code.lower() != 'default'
            }
            Specification[(m, norm)] = codes

    total = len(Specification)  # –≤–∫–ª—é—á–∞–µ—Ç endpoint-—ã —Å –ø—É—Å—Ç—ã–º Specification

    # 2) –°—á–∏—Ç–∞–µ–º fully covered:
    #    - empty Specification ‚Üí —Å—á–∏—Ç–∞–µ–º –ø–æ–∫—Ä—ã—Ç—ã–º
    #    - –∏–Ω–∞—á–µ Specification ‚äÜ used_status_codes
    fully = 0
    for ep, exp_codes in Specification.items():
        if not exp_codes:
            # –≤ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ –±—ã–ª–æ —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–æ–¥–æ–≤ ‚Üí —Å—á–∏—Ç–∞–µ–º –ø–æ–∫—Ä—ã—Ç—ã–º
            fully += 1
        else:
            recv = used_status_codes.get(ep, set())
            if exp_codes.issubset(recv):
                fully += 1

    pct = (fully / total * 100) if total else 0.0
    print(f"API Endpoint Coverage: {fully}/{total} ({pct:.1f}%)")





def detect_flaky_tests(test_names: List[str], module_path: str = None, repeat: int = 3) -> List[str]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Å—Ç—ã —á–µ—Ä–µ–∑ pytest –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Ñ–ª–∞–∫–∏-—Ç–µ—Å—Ç–æ–≤.
    """
    flaky: List[str] = []
    for name in test_names:
        results = []
        for i in range(repeat):
            report_file = f"report_{name}_{i}.json"
            report = run_pytest_json(test_name=name, report_file=report_file)
            tests = report.get("tests", [])
            outcome = tests[0].get("outcome") if tests else "failed"
            results.append(outcome == "passed")
            # Clean up report file
            if os.path.exists(report_file):
                os.remove(report_file)
        if len(set(results)) > 1:
            flaky.append(name)
    print(f"‚ö†Ô∏è Flaky tests: {flaky or ['None']}")
    return flaky


def fetch_spec(base_url: str, paths: List[str]) -> bytes | None:
    for path in paths:
        url = base_url.rstrip('/') + path
        try:
            r = requests.get(url, timeout=5)
            ctype = r.headers.get('Content-Type', '')
            if r.status_code == 200 and ('application/json' in ctype or r.text.lstrip().startswith(('{', 'swagger:'))):
                print(f"‚úÖ Spec found at: {url}")
                return r.content
        except requests.RequestException:
            pass
    print("‚ùå Spec not found on standard paths.")
    return None


# def calculate_cyclomatic_complexity(path: str) -> int:
#     code = open(path, 'r', encoding='utf-8').read()
#     visitor = ComplexityVisitor()
#     visitor.visit(ast.parse(code))
#     max_c = 0
#     for func in visitor.functions:
#         max_c = max(max_c, func.complexity)
#     print(f" Cyclomatic complexity: {max_c}")
#     return max_c


def analyze_style(path: str) -> float:
    # –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –≤–µ—Å—å –≤—ã–≤–æ–¥ Pylint –≤ –±—É—Ñ–µ—Ä –∏ –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –µ–≥–æ
    buffer = StringIO()
    reporter = TextReporter(buffer)
    run = Run([path, '--score=yes'], reporter=reporter, exit=False)
    score = run.linter.stats.global_note
    print(f"Style Score: {score:.1f}/10")
    return score


if __name__ == "__main__":
    for TEST_FILE in TEST_FILEs:
        print(TEST_FILE)
        # 1) –ü–æ–ª—É—á–∞–µ–º –∏ —Ä–∞–∑–±–∏—Ä–∞–µ–º spec
        spec_bytes = fetch_spec(BASE_URL, ENDPOINTS)
        parsed = parse_openapi(spec_bytes) if spec_bytes else {}

        # 8) Pylint
        analyze_style(TEST_FILE)


        # 2) –ó–∞–ø—É—Å–∫–∞–µ–º pytest –∏ —Å—á–∏—Ç–∞–µ–º pass/fail
        report = run_pytest_json()
        get_pass_fail_rate_firs(report)

        # 3) –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –≤—ã–∑–æ–≤—ã –∏ –∫–æ–¥—ã
        used = extract_used_endpoints(TEST_FILE)
        used_status_codes = extract_used_status_codes(TEST_FILE)

        # 6) –ü–æ–ª–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ **–ø–æ —Å—Ç–∞—Ç—É—Å-–∫–æ–¥–∞–º**
        measure_full_status_coverage(used_status_codes, parsed)


        # –ü–æ–∫—Ä—ã—Ç–∏–µ –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º
        show_status_code_coverage_sec(used_status_codes, parsed)
        
        # 5) –ß–∞—Å—Ç–∏—á–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ **–ø–æ endpoint-–∞–º** (–º–µ—Ç–æ–¥+–ø—É—Ç—å)
        measure_api_coverage(used_status_codes, parsed)
        analyze_unSpecification_status_codes(TEST_FILE, parsed)

        get_pass_fail_rate_sec(report)


        # 4) –¢–∞–±–ª–∏—Ü–∞ –ø–æ –≤—Å–µ–º status-–∫–æ–¥–∞–º
        show_status_code_coverage(used_status_codes, parsed)
        analyze_unSpecification_status_det(TEST_FILE, parsed)
        print_pass_fail_details(report)


        # –î–æ–ª–≥–æ, –Ω–æ –Ω–∞–¥–µ–∂–Ω–æ
        # names = extract_test_names(TEST_FILE)
        # detect_flaky_tests(names)

        print("\n"*3)